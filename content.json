{"pages":[{"title":"about","text":"","link":"/about/index.html"},{"title":"tags","text":"","link":"/tags/index.html"},{"title":"categories","text":"","link":"/categories/index.html"},{"title":"contact","text":"","link":"/contact/index.html"},{"title":"friends","text":"","link":"/friends/index.html"}],"posts":[{"title":"Pandas数据处理-map、apply、applymap详解","text":"一盏灯， 一片昏黄； 一简书， 一杯淡茶。 守着那一份淡定， 品读属于自己的寂寞。 保持淡定， 才能欣赏到最美丽的风景！ 保持淡定， 人生从此不再寂寞。 在日常的数据处理中，经常会对一个DataFrame进行逐行、逐列和逐元素的操作，对应这些操作，Pandas中的map,apply和applymap可以解决绝大部分这样的数据处理需求。这篇文章就以案例附带图解的方式，为大家详细介绍一下这三个方法的实现原理，相信读完本文后，不论是小白还是Pandas的进阶学习者，都会对这三个方法有更深入的理解。 boolean=[True,False] gender=[\"男\",\"女\"] color=[\"white\",\"black\",\"yellow\"] data=pd.DataFrame({ \"height\":np.random.randint(150,190,100), \"weight\":np.random.randint(40,90,100), \"smoker\":[boolean[x] for x in np.random.randint(0,2,100)], \"gender\":[gender[x] for x in np.random.randint(0,2,100)], \"age\":np.random.randint(15,90,100), \"color\":[color[x] for x in np.random.randint(0,len(color),100) ] } ) 数据集如下所示，各列分别代表身高、体重、是否吸烟、性别、年龄和肤色 Series数据处理 如果需要把数据集中 gender 列的男替换为1，女替换为0，怎么做呢？绝对不是用for循环实现，使 用 Series.map() 可以很容易做到，少仅需一行代码。 #①使用字典进行映射 data[\"gender\"] = data[\"gender\"].map({\"男\":1, \"女\":0}) #②使用函数 def gender_map(x): gender = 1 if x == \"男\" else 0 return gender #注意这里传入的是函数名，不带括号 data[\"gender\"] = data[\"gender\"].map(gender_map) 不论是利用字典还是函数进行映射，方法都是把对应的数据逐个当作参数传入到字典或函数中，得到映射后的值。 apply 同时Series对象还有 apply 方法， apply 方法的作用原理和 map 方法类似，区别在于 apply 能够传入 功能更为复杂的函数。怎么理解呢？一起看看下面的例子。 假设在数据统计的过程中，年龄 age 列有较大误差，需要对其进行调整（加上或减去一个值）， 由于这个加上或减去的值未知，故在定义函数时，需要加多一个参数 bias ，此时用 map 方法是操 作不了的（传入 map 的函数只能接收一个参数）， apply 方法则可以解决这个问题 def apply_age(x,bias): return x+bias #以元组的方式传入额外的参数 data[\"age\"] = data[\"age\"].apply(apply_age,args=(-3,)) 可以看到age列都减了3，当然，这里只是简单举了个例子，当需要进行复杂处理时，更能体现 apply 的作用。总而言之，对于Series而言， map 可以解决绝大多数的数据处理需求，但如果需要使用较为复杂的 函数，则需要用到 apply 方法。 DataFrame数据处理 apply 对 DataFrame 而言， apply 是非常重要的数据处理方法，它可以接收各种各样的函数（Python内置 的或自定义的），处理方式很灵活，下面通过几个例子来看看 apply 的具体使用及其原理。 在进行具体介绍之前，首先需要介绍一下 DataFrame 中 axis 的概念，在 DataFrame 对象的大多数方法 中，都会有 axis 这个参数，它控制了你指定的操作是沿着0轴还是1轴进行。 axis=0 代表操作对 列 columns 进行， axis=1 代表操作对 行row 进行，如下图所示。 假设现在需要对 data 中的数值列分别进行取对数和求和的操作，这时可以用 apply 进行相应的操 作，因为是对列进行操作，所以需要指定 axis=0 ，使用下面的两行代码可以很轻松地解决我们的 问题。 # 沿着0轴求和 data[[\"height\",\"weight\",\"age\"]].apply(np.sum, axis=0) # 沿着0轴取对数 data[[\"height\",\"weight\",\"age\"]].apply(np.log, axis=0) 当沿着 轴0（axis=0） 进行操作时，会将各列( columns )默认以 Series 的形式作为参数，传入到你指定 的操作函数中，操作后合并并返回相应的结果。那如果在实际使用中需要按行进行操作（ axis=1 ）,那整个过程又是怎么实现的呢？在数据集中，有身高和体重的数据，所以根据这个，我们可以计算每个人的BMI指数（体检时常 用的指标，衡量人体肥胖程度和是否健康的重要标准），计算公式是： 体重指数BMI=体重/身高的平方 （国际单位kg/㎡） ，因为需要对每个样本进行操作，这里使用 axis=1 的 apply 进行操作，代码如下： def BMI(series): weight = series[\"weight\"] height = series[\"height\"]/100 BMI = weight/height**2 return BMI data[\"BMI\"] = data.apply(BMI,axis=1) 当 apply 设置了 axis=1 对行进行操作时，会默认将每一行数据以 Series 的形式（Series的索引为列 名）传入指定函数，返回相应的结果。总结一下对 DataFrame 的 apply 操作： 当 axis=0 时，对 每列columns 执行指定函数；当 axis=1 时，对 每行row 执行指定函数。 2. 无论 axis=0 还是 axis=1 ，其传入指定函数的默认形式均为 Series ，可以通过设置 raw=True 传入 numpy数组 。 3. 对每个Series执行结果后，会将结果整合在一起返回（若想有返回值，定义函数时需要 return 相应的值） 4. 当然， DataFrame 的 apply 和 Series 的 apply 一样，也能接收更复杂的函数，如传入参数等，实现 原理是一样的，具体用法详见官方文档。applymap applymap 的用法比较简单，会对 DataFrame 中的每个单元格执行指定函数的操作，虽然用途不如 apply 广泛，但在某些场合下还是比较有用的，如下面这个例子。 df = pd.DataFrame( { \"A\":np.random.randn(5), \"B\":np.random.randn(5), \"C\":np.random.randn(5), \"D\":np.random.randn(5), \"E\":np.random.randn(5), } ) 将所有的值保留两位小数显示，使用applymap df.applymap(lambda x:\"%.2f\" % x) Powered by MaSheng","link":"/2020/02/07/pandas-shu-ju-chu-li-map-apply-applymap-xiang-jie/"},{"title":"HA-Hadoop","text":"高可用hadoop集群 在Hadoop2.0之前，只有一个NameNode，若NameNode机器出现故障，那么整个集群都无法使用。这个架构存在单点故障的隐患。之后推出了HA的架构，即有两个NameNode，一台为active状态，一台为standby状态。active NameNode对外提供服务，standby实时同步了active NameNode的元数据，当active NameNode节点出现故障，standby NameNode节点可立即切换为active状态并对外提供服务。所以，在实际生产中一般采用HA架构。这里用三台机器测试搭建Hadoop高可用集群。 避坑–配置隔离机制shell(/bin/true) Host 安装软件 进程 hadoop001 hadoop、zookeeper NameNode、DFSZKFailoverController、JournalNode、DataNode 、 ResourceManager 、JobHistoryServer、NodeManager 、QuorumPeerMain hadoop002 hadoop、zookeeper NameNode 、DFSZKFailoverController、JournalNode 、DataNode 、ResourceManager 、NodeManager 、QuorumPeerMain hadoop003 hadoop、zookeeper JournalNode 、DataNode 、NodeManager、QuorumPeerMain  修改配置文件 cd /usr/local/src/hadoop-2.6.0/etc/hadoop hadoop-env.sh export JAVA_HOME=/opt/apps/jdk1.8.0_172core-site.xmlmkdir –p /usr/local/src/hadoop-2.6.0/tmp &lt;configuration> &lt;property> &lt;name>fs.defaultFS&lt;/name> &lt;value>hdfs://ns&lt;/value> &lt;/property> &lt;property> &lt;name>hadoop.tmp.dir&lt;/name> &lt;value>/usr/local/src/hadoop-2.6.0/tmp&lt;/value> &lt;/property> &lt;property> &lt;name>ha.zookeeper.quorum&lt;/name> &lt;value>master:2181,slave1:2181,slave2:2181&lt;/value> &lt;/property> &lt;/configuration> hdfs-site.xml &lt;!--指定hdfs的nameservice为ns，需要和core-site.xml中的保持一致 --> &lt;property> &lt;name>dfs.nameservices&lt;/name> &lt;value>ns&lt;/value> &lt;/property> &lt;!-- ns下面有两个NameNode，分别是nn1，nn2 --> &lt;property> &lt;name>dfs.ha.namenodes.ns&lt;/name> &lt;value>nn1,nn2&lt;/value> &lt;/property> &lt;!-- nn1的RPC通信地址 --> &lt;property> &lt;name>dfs.namenode.rpc-address.ns.nn1&lt;/name> &lt;value>master:9000&lt;/value> &lt;/property> &lt;!-- nn1的http通信地址 --> &lt;property> &lt;name>dfs.namenode.http-address.ns.nn1&lt;/name> &lt;value>master:50070&lt;/value> &lt;/property> &lt;!-- nn2的RPC通信地址 --> &lt;property> &lt;name>dfs.namenode.rpc-address.ns.nn2&lt;/name> &lt;value>slave1:9000&lt;/value> &lt;/property> &lt;!-- nn2的http通信地址 --> &lt;property> &lt;name>dfs.namenode.http-address.ns.nn2&lt;/name> &lt;value>slave1:50070&lt;/value> &lt;/property> &lt;!-- 指定NameNode的元数据在JournalNode上的存放位置 --> &lt;property> &lt;name>dfs.namenode.shared.edits.dir&lt;/name> &lt;value>qjournal://master:8485;slave1:8485;slave2:8485/ns&lt;/value> &lt;/property> &lt;!-- 指定JournalNode在本地磁盘存放数据的位置 --> &lt;property> &lt;name>dfs.journalnode.edits.dir&lt;/name> &lt;value>/usr/local/src/hadoop-2.6.0/hdf_journal&lt;/value> &lt;/property> &lt;!-- 开启NameNode故障时自动切换 --> &lt;property> &lt;name>dfs.ha.automatic-failover.enabled&lt;/name> &lt;value>true&lt;/value> &lt;/property> &lt;!-- 配置失败自动切换实现方式 --> &lt;property> &lt;name>dfs.client.failover.proxy.provider.ns&lt;/name> &lt;value>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider&lt;/value> &lt;/property> &lt;!-- 配置隔离机制 --> &lt;property> &lt;name>dfs.ha.fencing.methods&lt;/name> &lt;value> sshfence shell(/bin/true) &lt;/value> &lt;/property> &lt;!-- 使用隔离机制时需要ssh免登陆 --> &lt;property> &lt;name>dfs.ha.fencing.ssh.private-key-files&lt;/name> &lt;value>/root/.ssh/id_rsa&lt;/value> &lt;/property> &lt;!-- 配置sshfence隔离机制超时时间 --> &lt;property> &lt;name>dfs.ha.fencing.ssh.connect-timeout&lt;/name> &lt;value>30000&lt;/value> &lt;/property> &lt;property> &lt;name>dfs.namenode.name.dir&lt;/name> &lt;value>/usr/local/src/hadoop-2.6.0/hdf_name&lt;/value> &lt;final>true&lt;/final> &lt;/property> &lt;property> &lt;name>dfs.datanode.data.dir&lt;/name> &lt;value>/usr/local/src/hadoop-2.6.0/hdf_data&lt;/value> &lt;/property> &lt;property> &lt;name>dfs.replication&lt;/name> &lt;value>3&lt;/value> &lt;/property> &lt;!-- 在NN和DN上开启WebHDFS (REST API)功能,不是必须 --> &lt;property> &lt;name>dfs.webhdfs.enabled&lt;/name> &lt;value>true&lt;/value> &lt;/property> mapred-site.xml cp mapred-site.xml.template mapred-site.xml &lt;property> &lt;name>mapreduce.framework.name&lt;/name> &lt;value>yarn&lt;/value> &lt;/property> yarn-site.xml &lt;!-- 开启YARN HA --> &lt;property> &lt;name>yarn.resourcemanager.ha.enabled&lt;/name> &lt;value>true&lt;/value> &lt;/property> &lt;!-- 启用自动故障转移 --> &lt;property> &lt;name>yarn.resourcemanager.ha.automatic-failover.enabled&lt;/name> &lt;value>true&lt;/value> &lt;/property> &lt;!-- 指定YARN HA的名称 --> &lt;property> &lt;name>yarn.resourcemanager.cluster-id&lt;/name> &lt;value>yarncluster&lt;/value> &lt;/property> &lt;!-- 指定两个resourcemanager的名称 --> &lt;property> &lt;name>yarn.resourcemanager.ha.rm-ids&lt;/name> &lt;value>rm1,rm2&lt;/value> &lt;/property> &lt;!-- 配置rm1，rm2的主机 --> &lt;property> &lt;name>yarn.resourcemanager.hostname.rm1&lt;/name> &lt;value>slave2&lt;/value> &lt;/property> &lt;property> &lt;name>yarn.resourcemanager.hostname.rm2&lt;/name> &lt;value>slave1&lt;/value> &lt;/property> &lt;!-- 配置YARN的http端口 --> &lt;property> &lt;name>yarn.resourcemanager.webapp.address.rm1&lt;/name> &lt;value>slave2:8088&lt;/value> &lt;/property> &lt;property> &lt;name>yarn.resourcemanager.webapp.address.rm2&lt;/name> &lt;value>slave1:8088&lt;/value> &lt;/property> &lt;!-- 配置zookeeper的地址 --> &lt;property> &lt;name>yarn.resourcemanager.zk-address&lt;/name> &lt;value>master:2181,slave1:2181,slave2:2181&lt;/value> &lt;/property> &lt;!-- 配置zookeeper的存储位置 --> &lt;property> &lt;name>yarn.resourcemanager.zk-state-store.parent-path&lt;/name> &lt;value>/rmstore&lt;/value> &lt;/property> &lt;!-- 开启yarn resourcemanager restart --> &lt;property> &lt;name>yarn.resourcemanager.recovery.enabled&lt;/name> &lt;value>true&lt;/value> &lt;/property> &lt;!-- 配置resourcemanager的状态存储到zookeeper中 --> &lt;property> &lt;name>yarn.resourcemanager.store.class&lt;/name> &lt;value>org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore&lt;/value> &lt;/property> &lt;!-- 开启yarn nodemanager restart --> &lt;property> &lt;name>yarn.nodemanager.recovery.enabled&lt;/name> &lt;value>true&lt;/value> &lt;/property> &lt;property> &lt;name>yarn.nodemanager.aux-services&lt;/name> &lt;value>mapreduce_shuffle&lt;/value> &lt;/property> &lt;property> &lt;name>yarn.nodemanager.address&lt;/name> &lt;value>${yarn.nodemanager.hostname}:8041&lt;/value> &lt;/property>  修改slaves文件 master slave1 slave2  拷贝到其他环境 scp -r /usr/local/src/hadoop-2.6.0 slave1:/usr/local/src/ scp -r /usr/local/src/hadoop-2.6.0 slave2:/usr/local/src/","link":"/2020/02/07/ha-hadoop-pei-zhi-wen-jian/"},{"title":"Hello World","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post$ hexo new \"My New Post\" More info: Writing Run server$ hexo server More info: Server Generate static files$ hexo generate More info: Generating Deploy to remote sites$ hexo deploy More info: Deployment","link":"/2020/02/07/hello-world/"},{"title":"centos7安装Mysql5.7","text":"要大笑、要做梦、要与众不同。人生是一场伟大的冒险。——《外婆的道歉信》 为yum配置mysql57的源 # 下载MySQL的repository wget -i -c http://dev.mysql.com/get/mysql57-community-release-el7-10.noarch.rpm # 安装MySQL源 yum -y install mysql57-community-release-el7-10.noarch.rpm # 通过yum安装MySQL yum -y install mysql-community-server 启动mysql # 启动MySQL systemctl start mysqld.service # 查看其状态 systemctl status mysqld.service 看到 active(running) 即为正确运行，如下 查看默认密码 grep \"password\" /var/log/mysqld.log #查看结果如下： #登录 mysql -u root -p输入默认密码关闭弱密码限制和长度限制 mysql> set global validate_password_policy=0; mysql> set global validate_password_length=1; 修改密码 mysql> ALTER USER 'root'@'localhost' IDENTIFIED BY '123456'; /* 比较新的MySQL 要使用alter user 来修改密码 */ 开启 mysql 中 root用户远程访问权限 查看默认设置 mysql> use mysql; mysql> select host,user from user; 可知，root用户只能本地访问： 开启root远程访问权限 mysql> GRANT ALL PRIVILEGES ON *.* TO 'root'@'%' IDENTIFIED BY '123456' WITH GRANT OPTION; /* % 表示全部允许ip root为用户 123456为密码 */ mysql> flush privileges; /* 刷新，或者重新启动mysql也行 */ 再次查看 mysql> select host,user from user; 摘录，引用： https://blog.csdn.net/u010360923/article/details/89673406","link":"/2020/02/09/centos7-an-zhuang-mysql5-7/"},{"title":"hive的安装","text":"《美国往事》里有句台词说：”当我对世事厌倦的时候，我就会想到你。想到你在世界的某个地方生活着，存在着，我就愿意忍受一切。你的存在对我来说，很重要。 安装hive之前，请完成： hadoop的安装配置 mysql的安装配置 hdfs的启动 不推荐mysql8,因为hive版本较旧 在大多数的文章中，都是用cp hive-default.xml.template hive-site.xml的方式来配置hive-site.xml我认为很不科学，因为这个文件有上万行，改动起来很麻烦，所以换了一种赋值方式 下载、配置Hive 下载 wget https://mirrors.aliyun.com/apache/hive/hive-2.3.6/apache-hive-2.3.6-bin.tar.gz 安装Hive，并设置环境变量 # 解压Hive 到安装目录/usr/local/ tar -zxvf apache-hive*.tar.gz -C /usr/local/ # 编辑profile文件 配置Hive环境变量 vim /etc/profile # 配置的内容 # Hive export HIVE_HOME=/usr/local/apache-hive-2.3.6-bin export HIVE_CONF_DIR=$HIVE_HOME/conf export PATH=$HIVE_HOME/bin:$PATH # 记得source一下,使其生效 source /etc/profile cd /usr/local/hive/conf mv hive-default.xml.template hive-default.xml # 基于模板创建hive-env.sh cp hive-env.sh.template hive-env.sh cd /usr/local/hive/conf vim hive-site.xml添加如下内容 &lt;configuration> &lt;property> &lt;name>hive.metastore.warehouse.dir&lt;/name> &lt;value>/user/hive/warehouse&lt;/value> &lt;description>location of default database for the warehouse&lt;/description> &lt;/property> &lt;property> &lt;name>hive.exec.scratchdir&lt;/name> &lt;value>/tmp/hive&lt;/value> &lt;description>HDFS root scratch dir for Hive jobs which gets created with write all (733) permission. For each connecting user, an HDFS scratch dir: ${hive.exec.scratchdir}/&amp;lt;username&amp;gt; is created, with ${hive.scratch.dir.permission}.&lt;/description> &lt;/property> &lt;/configuration> 根据 hive-site.xml 中的上述的两条配置内容,我们需要在hadoop中建立两个目录： # 在hdfs中建立/user/hive/warehouse并设置权限 hadoop fs -mkdir -p /user/hive/warehouse hadoop fs -chmod -R 777 /user/hive/warehouse # 在hdfs中建立/tmp/hive/并设置权限 hadoop fs -mkdir -p /tmp/hive/ hadoop fs -chmod -R 777 /tmp/hive 通过浏览器查看公网ip:50070端口（Hadoop3.x.y端口为9870），查看HDFS是否存在新建的目录 JAVA.IO的临时目录,我将其设定为hive路径下的tmp目录中，因为tmp不存在所以需要新建tmp # 进入到hive的安装目录 cd /usr/local/apache-hive-2.3.6-bin mkdir tmp chmod -R 777 tmp/ 设置system:java.io.tmpdir 在hive-site.xml继续添加如下： &lt;property> &lt;name>java.io.tmpdir&lt;/name> &lt;value>/usr/local/apache-hive-2.3.6-bin/tmp&lt;/value> &lt;/property> &lt;property> &lt;name>user.name&lt;/name> &lt;value>root&lt;/value> &lt;/property> &lt;property> &lt;name>javax.jdo.option.ConnectionDriverName&lt;/name> &lt;value>com.mysql.jdbc.Driver&lt;/value> &lt;description>Driver class name for a JDBC metastore&lt;/description> &lt;/property> &lt;property> &lt;name>javax.jdo.option.ConnectionURL&lt;/name> &lt;value>jdbc:mysql://换成你的mysql主机IP地址:3306/hive?createDatabaseIfNotExist=true&amp;amp;characterEncoding=UTF-8&amp;amp;useSSL=false&lt;/value> &lt;/property> &lt;property> &lt;name>javax.jdo.option.ConnectionUserName&lt;/name> &lt;value>root&lt;/value> &lt;description>上面的root为MySQL数据库登录名&lt;/description> &lt;/property> &lt;property> &lt;name>javax.jdo.option.ConnectionPassword&lt;/name> &lt;value>123456&lt;/value> &lt;description>上面的123456为MySQL数据库密码&lt;/description> &lt;/property> javax.jdo.option.ConnectionDriverName 对应的value修改为MySQL驱动类路径注：此处使用的jdbc版本是mysql-connector-java-5.1.39-bin.jar，如果你的版本是8或者更高，请加入预处理。即，将com.mysql.jdbc.Driver改为com.mysql.cj.jdbc.Driver hive-env.sh配置文件 在hive的conf目录下，打开配置文件 vim hive-env.sh配置HIVE_AUX_JARS_PATH, HIVE_CONF_DIR, HADOOP_HOME在已有的基础上修改如下： #Folder containing extra libraries required for hive compilation/execution can be controlled by: #export HIVE_AUX_JARS_PATH= export HIVE_AUX_JARS_PATH=/usr/local/apache-hive-2.3.6-bin/lib #Hive Configuration Directory can be controlled by: #export HIVE_CONF_DIR= export HIVE_CONF_DIR=/usr/local/apache-hive-2.3.6-bin/conf #Set HADOOP_HOME to point to a specific hadoop install directory #HADOOP_HOME=${bin}/../../hadoop export HADOOP_HOME=/usr/local/hadoop 下载jdbc驱动 wget http://mirrors.ustc.edu.cn/mysql-ftp/Downloads/Connector-J/mysql-connector-java-5.1.48.tar.gz tar -zxvf mysql-connector-java-5.1.48.tar.gz #解压 cp mysql-connector-java-5.1.48/mysql-connector-java-5.1.48-bin.jar /usr/local/hive/lib #将mysql-connector-java-5.1.40-bin.jar拷贝到/usr/local/hive/lib目录下 初始化mysql的hive数据库 #对数据库进行初始化 schematool -initSchema -dbType mysql #启动 hive 建库 hive>create database test; web查看Hive 通过 SQL 查询语句实现 wordcount https://blog.csdn.net/u010360923/article/details/90341102","link":"/2020/02/09/hive-de-an-zhuang/"},{"title":"云服务器的伪分布hadoop","text":"我希望自己能够有足够的运气和足够的勇气去见到命运里更多的不同的风。也期待在未来的日子里，能够被这些涌动的气流雕刻成不一样的山川与河流。–《见风》 厦门大学数据库实验室网站提供了阿里云构建大数据环境的详细流程，从服务器的配置到ubuntu的配置到大数据环境的搭建 详情见dblab.xmu.edu 一、安装jdk 下载jdk （提取码：q4zl）点击这里下载 上传到/home下，解压缩 tar -zxvf /home/jdk-8u45-linux-x64.tar.gz -C /usr/local/ 将Java添加至环境变量 vim /etc/profile 添加如下： export JAVA_HOME=/usr/local/jdk1.8.0_45 export PATH=$PATH:$JAVA_HOME/bin source /etc/profile 生效 java -version 检测一下 systemctl status firewalld 检查下防火墙状态 (如果为active状态,执行 systemctl stop firewalld systemctl disable firewalld)二 、环境配置 修改hostname vim /etc/hostname 修改为hadoop hostnamectl set-hostname hadoop 立即生效修改映射关系 vim /etc/hosts 127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4 ::1 localhost localhost.localdomain localhost6 localhost6.localdomain6 172.19.199.187 hadoop 注意：172.19.199.187改成你的私网ip！！！（私网ip阿里云控制台上有显示） 创建用户hadoop adduser hadoop 创建用户 passwd hadoop 修改密码 给hadoop用户root权限 vim /etc/sudoers 添加一行 hadoop ALL=(root) NOPASSWD:ALL设置ssh 生成密钥 ssh-keygen -t rsa -P &quot;&quot; 将密钥写入ssh中，这样在本地启动时，就可以避免输入密码 cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys三、Hadoop伪分布式环境搭建 下载hadoop,在/home下执行 wget https://mirrors.aliyun.com/apache/hadoop/common/hadoop-2.8.5/hadoop-2.8.5.tar.gz 解压 tar -zxvf /home/hadoop-2.8.5.tar.gz -C /usr/local/ 改名 mv hadoop-2.8.5 hadoop 修改文件权限 sudo chown -R hadoop:hadoop /usr/local/hadoop修改hadoop/etc/hadoop/hadoop-env.sh文件 export JAVA_HOME=/usr/local/jdk1.8.0_45 修改hadoop/etc/hadoop/core-site.xml文件 &lt;configuration> &lt;property> &lt;name>hadoop.tmp.dir&lt;/name> &lt;value>file:/usr/local/hadoop/tmp&lt;/value> &lt;description>Abase for other temporary directories.&lt;/description> &lt;/property> &lt;property> &lt;name>fs.defaultFS&lt;/name> &lt;value>hdfs://172.19.199.187:9000&lt;/value> &lt;/property> &lt;/configuration> 注意：172.19.199.187改成你的私网ip！！！（私网ip阿里云控制台上有显示） 修改etc/hadoop/hdfs-site.xml文件 指定HDFS文件存储的副本数个数，默认是3个，这里是单台机器就设置为1，这个数字要小于datanode的节点数 &lt;configuration> &lt;property> &lt;name>dfs.replication&lt;/name> &lt;value>1&lt;/value> &lt;/property> &lt;property> &lt;name>dfs.namenode.name.dir&lt;/name> &lt;value>file:/usr/local/hadoop/tmp/dfs/name&lt;/value> &lt;/property> &lt;property> &lt;name>dfs.datanode.data.dir&lt;/name> &lt;value>file:/usr/local/hadoop/tmp/dfs/data&lt;/value> &lt;/property> &lt;!-- 如果是通过公网IP访问阿里云上内网搭建的集群 --> &lt;property> &lt;name>dfs.client.use.datanode.hostname&lt;/name> &lt;value>true&lt;/value> &lt;description>only cofig in clients&lt;/description> &lt;/property> &lt;/configuration> 将hadoop添加至环境变量 vim /etc/profile 添加如下： export HADOOP_HOME=/usr/local/hadoop export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin source /etc/profile 执行生效 hdfs namenode -format 格式化 start-dfs.sh 启动hdfs jps查看21923 Jps 21592 DataNode 21417 NameNode 21806 SecondaryNameNode 单机不用启动yarn 四 查看HDFS外部UI界面（阿里云) 开启服务器外网访问端口—-网络和安全-》安全组-》配置规则 添加安全组规则-》添加端口50070 访问web端口 http://公网ip:50070 查看live的datanode 上传一个文件执行mapreduce-wordcount测试 上传文件 hadoop fs -put /lipstick.csv / 执行 hadoop jar /usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.8.5.jar wordcount /lipstick.csv /output 阿里云上搭建的hadoop集群，需要配置映射集群经过内网访问，也就是局域网的ip地址。如果配置为公网IP地址，就会出现集群启动不了，namenode和secondarynamenode启动不了，如果将主机的映射文件配置为内网IP集群就可以正常启动了。但通过eclipse开发工具访问会出错，显示了阿里云内网的ip地址来访问datanode。 摘录引用： https://www.jianshu.com/p/18ded352cf40 http://dblab.xmu.edu.cn/blog/2022-2/ https://blog.csdn.net/u010886217/article/details/83479380","link":"/2020/02/08/yun-fu-wu-qi-de-wei-fen-bu-hadoop/"}],"tags":[{"name":"数据分析","slug":"数据分析","link":"/tags/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/"},{"name":"pandas","slug":"pandas","link":"/tags/pandas/"},{"name":"hadoop","slug":"hadoop","link":"/tags/hadoop/"},{"name":"高可用","slug":"高可用","link":"/tags/%E9%AB%98%E5%8F%AF%E7%94%A8/"},{"name":"centos7","slug":"centos7","link":"/tags/centos7/"},{"name":"mysql","slug":"mysql","link":"/tags/mysql/"},{"name":"hive","slug":"hive","link":"/tags/hive/"},{"name":"mysql模式","slug":"mysql模式","link":"/tags/mysql%E6%A8%A1%E5%BC%8F/"},{"name":"阿里云","slug":"阿里云","link":"/tags/%E9%98%BF%E9%87%8C%E4%BA%91/"},{"name":"伪分布","slug":"伪分布","link":"/tags/%E4%BC%AA%E5%88%86%E5%B8%83/"}],"categories":[{"name":"数据分析","slug":"数据分析","link":"/categories/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/"},{"name":"配置","slug":"配置","link":"/categories/%E9%85%8D%E7%BD%AE/"}]}